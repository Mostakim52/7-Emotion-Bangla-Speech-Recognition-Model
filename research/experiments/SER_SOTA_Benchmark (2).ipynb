{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speech Emotion Recognition - SOTA Model Benchmark\n",
        "## Using Existing Data Pipeline from Russian-Hacker.ipynb\n",
        "\n",
        "This notebook benchmarks multiple SOTA models while keeping the **exact same** data loading and preprocessing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 1: Imports and Configuration (SAME AS YOUR NOTEBOOK)\n",
        "# =============================================\n",
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration (SAME AS YOUR NOTEBOOK)\n",
        "SUBESCO_PATH = 'H:/'\n",
        "BANGLA_SER_PATH = 'H:/'\n",
        "BANSPEMO_PATH = 'H:/A Bangla Language Emotional Speech Recognition Dataset/A Bangla Language Emotional Speech Recognition Dataset/Dataset'\n",
        "\n",
        "EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
        "SAMPLE_RATE = 16000\n",
        "DURATION = 3\n",
        "N_MFCC = 40\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 50\n",
        "\n",
        "# Emotion mappings (SAME AS YOUR NOTEBOOK)\n",
        "SUBESCO_EMOTIONS = {\n",
        "    'ANGRY': 'Angry', 'DISGUST': 'Disgust', 'FEAR': 'Fear',\n",
        "    'HAPPY': 'Happy', 'NEUTRAL': 'Neutral', 'SAD': 'Sad', 'SURPRISE': 'Surprise'\n",
        "}\n",
        "\n",
        "BANGLA_SER_EMOTIONS = {\n",
        "    '01': 'Happy', '02': 'Sad', '03': 'Angry', '04': 'Surprise', '05': 'Neutral'\n",
        "}\n",
        "\n",
        "BANSPEMO_EMOTIONS = {\n",
        "    '01': 'Angry', '02': 'Disgust', '03': 'Fear',\n",
        "    '04': 'Happy', '05': 'Sad', '06': 'Surprise'\n",
        "}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 2: Feature Extraction Functions (SAME AS YOUR NOTEBOOK)\n",
        "# =============================================\n",
        "def extract_features(file_path):\n",
        "    \"\"\"Extract MFCC features - SAME as your notebook\"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC)\n",
        "        return mfcc.T\n",
        "    except Exception as e:\n",
        "        print(f'Error processing {file_path}: {e}')\n",
        "        return None\n",
        "\n",
        "def extract_raw_audio(file_path):\n",
        "    \"\"\"Extract raw audio for Wav2Vec2/HuBERT models\"\"\"\n",
        "    try:\n",
        "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "        # Pad or truncate to fixed length\n",
        "        target_length = SAMPLE_RATE * DURATION\n",
        "        if len(audio) < target_length:\n",
        "            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
        "        else:\n",
        "            audio = audio[:target_length]\n",
        "        return audio\n",
        "    except Exception as e:\n",
        "        print(f'Error processing {file_path}: {e}')\n",
        "        return None\n",
        "\n",
        "def get_subesco_emotion(filename):\n",
        "    parts = filename.split('_')\n",
        "    return SUBESCO_EMOTIONS.get(parts[5], None) if len(parts) > 5 else None\n",
        "\n",
        "def get_bangla_ser_emotion(filename):\n",
        "    parts = filename.split('-')\n",
        "    return BANGLA_SER_EMOTIONS.get(parts[2], None) if len(parts) >= 3 else None\n",
        "\n",
        "def get_banspemo_emotion(filename):\n",
        "    parts = filename.split('_')\n",
        "    return BANSPEMO_EMOTIONS.get(parts[-1].split('.')[0], None) if len(parts) >= 1 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 3: Data Loading (SAME AS YOUR NOTEBOOK)\n",
        "# =============================================\n",
        "def load_data(extract_raw=False):\n",
        "    \"\"\"Load data - SAME logic as your notebook, with option for raw audio\"\"\"\n",
        "    features = []\n",
        "    raw_audios = []\n",
        "    labels = []\n",
        "    \n",
        "    print('Processing SUBESCO dataset...')\n",
        "    for root, _, files in os.walk(SUBESCO_PATH):\n",
        "        for filename in files:\n",
        "            if filename.endswith('.wav'):\n",
        "                emotion = get_subesco_emotion(filename)\n",
        "                if emotion:\n",
        "                    filepath = os.path.join(root, filename)\n",
        "                    mfcc = extract_features(filepath)\n",
        "                    if mfcc is not None:\n",
        "                        features.append(mfcc)\n",
        "                        labels.append(emotion)\n",
        "                        if extract_raw:\n",
        "                            raw = extract_raw_audio(filepath)\n",
        "                            raw_audios.append(raw)\n",
        "    \n",
        "    print('Processing BANSPEMO dataset...')\n",
        "    for root, _, files in os.walk(BANSPEMO_PATH):\n",
        "        for filename in files:\n",
        "            if filename.endswith('.wav'):\n",
        "                emotion = get_banspemo_emotion(filename)\n",
        "                if emotion:\n",
        "                    filepath = os.path.join(root, filename)\n",
        "                    mfcc = extract_features(filepath)\n",
        "                    if mfcc is not None:\n",
        "                        features.append(mfcc)\n",
        "                        labels.append(emotion)\n",
        "                        if extract_raw:\n",
        "                            raw = extract_raw_audio(filepath)\n",
        "                            raw_audios.append(raw)\n",
        "    \n",
        "    print('Processing BANGLA_SER dataset...')\n",
        "    for actor_folder in os.listdir(BANGLA_SER_PATH):\n",
        "        actor_path = os.path.join(BANGLA_SER_PATH, actor_folder)\n",
        "        if not os.path.isdir(actor_path):\n",
        "            continue\n",
        "        for filename in os.listdir(actor_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                emotion = get_bangla_ser_emotion(filename)\n",
        "                if emotion:\n",
        "                    filepath = os.path.join(actor_path, filename)\n",
        "                    mfcc = extract_features(filepath)\n",
        "                    if mfcc is not None:\n",
        "                        features.append(mfcc)\n",
        "                        labels.append(emotion)\n",
        "                        if extract_raw:\n",
        "                            raw = extract_raw_audio(filepath)\n",
        "                            raw_audios.append(raw)\n",
        "    \n",
        "    # Pad sequences (SAME as your notebook)\n",
        "    max_len = max(len(f) for f in features)\n",
        "    features = np.array([np.pad(f, ((0, max_len - len(f)), (0, 0)), mode='constant') for f in features])\n",
        "    \n",
        "    # Encode labels (SAME as your notebook)\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(labels)\n",
        "    \n",
        "    if extract_raw:\n",
        "        raw_audios = np.array(raw_audios)\n",
        "        return features, raw_audios, labels, le\n",
        "    return features, labels, le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 4: Dataset Classes\n",
        "# =============================================\n",
        "class EmotionDataset(Dataset):\n",
        "    \"\"\"MFCC Dataset - SAME as your notebook\"\"\"\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        feature = torch.tensor(self.features[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return feature, label\n",
        "\n",
        "class RawAudioDataset(Dataset):\n",
        "    \"\"\"Raw Audio Dataset for Wav2Vec2/HuBERT\"\"\"\n",
        "    def __init__(self, raw_audios, labels):\n",
        "        self.raw_audios = raw_audios\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.raw_audios)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        audio = torch.tensor(self.raw_audios[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return audio, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 5: MODEL 1 - Your Original CNN-BiLSTM (SAME AS YOUR NOTEBOOK)\n",
        "# =============================================\n",
        "class EmotionModel(nn.Module):\n",
        "    \"\"\"Your original model - CNN + Bidirectional LSTM\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(EmotionModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        \n",
        "        self.lstm1 = nn.LSTM(128, hidden_size, num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_size*2, hidden_size, num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
        "        \n",
        "        self.fc1 = nn.Linear(hidden_size*2, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = x[:, -1, :]\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 6: MODEL 2 - CNN-Transformer Hybrid (FROM YOUR NOTEBOOK)\n",
        "# =============================================\n",
        "class CNNBackbones(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNBackbones, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv1d(N_MFCC, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        self.pool2 = nn.AdaptiveMaxPool1d(31)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.pool2(x)\n",
        "        return x\n",
        "\n",
        "class CNNTransformerHybrid(nn.Module):\n",
        "    \"\"\"CNN-Transformer Hybrid from your notebook\"\"\"\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(CNNTransformerHybrid, self).__init__()\n",
        "        self.cnn = CNNBackbones()\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=128, nhead=4, dim_feedforward=512, dropout=0.1, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(31 * 256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, time)\n",
        "        cnn_out = self.cnn(x)\n",
        "        transformer_in = cnn_out.permute(0, 2, 1)  # (batch, time, features)\n",
        "        transformer_out = self.transformer(transformer_in)\n",
        "        combined = torch.cat([cnn_out.permute(0, 2, 1), transformer_out], dim=-1)\n",
        "        combined = combined.reshape(combined.size(0), -1)\n",
        "        out = self.fc(combined)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 7: MODEL 3 - ECAPA-TDNN (SOTA for Speaker/Emotion Recognition)\n",
        "# =============================================\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation Block\"\"\"\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, c, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class Res2Block(nn.Module):\n",
        "    \"\"\"Res2Net-style block for ECAPA-TDNN\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, scale=8):\n",
        "        super(Res2Block, self).__init__()\n",
        "        self.scale = scale\n",
        "        width = out_channels // scale\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(width, width, kernel_size, dilation=dilation, padding=dilation*(kernel_size-1)//2)\n",
        "            for _ in range(scale - 1)\n",
        "        ])\n",
        "        self.bns = nn.ModuleList([nn.BatchNorm1d(width) for _ in range(scale - 1)])\n",
        "        \n",
        "        self.conv3 = nn.Conv1d(out_channels, out_channels, 1)\n",
        "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
        "        self.se = SEBlock(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        residual = self.shortcut(x)\n",
        "        \n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        \n",
        "        spx = torch.split(out, out.size(1) // self.scale, dim=1)\n",
        "        sp_outs = [spx[0]]\n",
        "        for i in range(1, self.scale):\n",
        "            if i == 1:\n",
        "                sp_out = spx[i]\n",
        "            else:\n",
        "                sp_out = sp_out + spx[i]\n",
        "            sp_out = self.relu(self.bns[i-1](self.convs[i-1](sp_out)))\n",
        "            sp_outs.append(sp_out)\n",
        "        out = torch.cat(sp_outs, dim=1)\n",
        "        \n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.se(out)\n",
        "        \n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ECAPA_TDNN(nn.Module):\n",
        "    \"\"\"ECAPA-TDNN for Speech Emotion Recognition\"\"\"\n",
        "    def __init__(self, input_size=40, num_classes=7, channels=512):\n",
        "        super(ECAPA_TDNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(input_size, channels, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        self.layer1 = Res2Block(channels, channels, kernel_size=3, dilation=2)\n",
        "        self.layer2 = Res2Block(channels, channels, kernel_size=3, dilation=3)\n",
        "        self.layer3 = Res2Block(channels, channels, kernel_size=3, dilation=4)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(channels * 3, channels * 3, kernel_size=1)\n",
        "        \n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv1d(channels * 3, 128, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv1d(128, channels * 3, kernel_size=1),\n",
        "            nn.Softmax(dim=2)\n",
        "        )\n",
        "        \n",
        "        self.bn2 = nn.BatchNorm1d(channels * 6)\n",
        "        self.fc = nn.Linear(channels * 6, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, time)\n",
        "        \n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        \n",
        "        out1 = self.layer1(x)\n",
        "        out2 = self.layer2(out1)\n",
        "        out3 = self.layer3(out2)\n",
        "        \n",
        "        out = torch.cat([out1, out2, out3], dim=1)\n",
        "        out = self.relu(self.conv2(out))\n",
        "        \n",
        "        # Attentive Statistics Pooling\n",
        "        alpha = self.attention(out)\n",
        "        mean = torch.sum(alpha * out, dim=2)\n",
        "        std = torch.sqrt((torch.sum(alpha * out ** 2, dim=2) - mean ** 2).clamp(min=1e-9))\n",
        "        out = torch.cat([mean, std], dim=1)\n",
        "        \n",
        "        out = self.bn2(out)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 8: MODEL 4 & 5 - Wav2Vec2 and HuBERT (Pretrained SOTA)\n",
        "# =============================================\n",
        "try:\n",
        "    from transformers import Wav2Vec2Model, HubertModel, Wav2Vec2Config, HubertConfig\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print('transformers not installed. Run: pip install transformers')\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "class Wav2Vec2ForSER(nn.Module):\n",
        "    \"\"\"Wav2Vec2 for Speech Emotion Recognition\"\"\"\n",
        "    def __init__(self, num_classes=7, pretrained='facebook/wav2vec2-base'):\n",
        "        super(Wav2Vec2ForSER, self).__init__()\n",
        "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(pretrained)\n",
        "        self.wav2vec2.freeze_feature_encoder()  # Freeze CNN encoder\n",
        "        \n",
        "        hidden_size = self.wav2vec2.config.hidden_size\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        outputs = self.wav2vec2(x)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        pooled = hidden_states.mean(dim=1)  # Mean pooling\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "class HuBERTForSER(nn.Module):\n",
        "    \"\"\"HuBERT for Speech Emotion Recognition\"\"\"\n",
        "    def __init__(self, num_classes=7, pretrained='facebook/hubert-base-ls960'):\n",
        "        super(HuBERTForSER, self).__init__()\n",
        "        self.hubert = HubertModel.from_pretrained(pretrained)\n",
        "        self.hubert.freeze_feature_encoder()\n",
        "        \n",
        "        hidden_size = self.hubert.config.hidden_size\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        outputs = self.hubert(x)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        pooled = hidden_states.mean(dim=1)\n",
        "        return self.classifier(pooled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 9: Training Function (SAME LOGIC AS YOUR NOTEBOOK)\n",
        "# =============================================\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, model_name):\n",
        "    \"\"\"Training loop - SAME logic as your notebook\"\"\"\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "    best_val_acc = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        \n",
        "        for features, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=False):\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        \n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for features, labels in val_loader:\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = 100 * correct / total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        \n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f'{model_name}_best.pth')\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "    \n",
        "    return train_losses, val_losses, train_accs, val_accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 10: Evaluation Function\n",
        "# =============================================\n",
        "def evaluate_model(model, test_loader, le, model_name):\n",
        "    \"\"\"Evaluate model and return metrics\"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    inference_times = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for features, labels in test_loader:\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "            \n",
        "            start_time = time.time()\n",
        "            outputs = model(features)\n",
        "            inference_times.append((time.time() - start_time) * 1000 / features.size(0))  # ms per sample\n",
        "            \n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted') * 100\n",
        "    precision = precision_score(y_true, y_pred, average='weighted') * 100\n",
        "    recall = recall_score(y_true, y_pred, average='weighted') * 100\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "    \n",
        "    print(f'\\n=== {model_name} Results ===')\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    print(f'F1-Score: {f1:.2f}%')\n",
        "    print(f'Precision: {precision:.2f}%')\n",
        "    print(f'Recall: {recall:.2f}%')\n",
        "    print(f'Avg Inference Time: {avg_inference_time:.2f} ms/sample')\n",
        "    print('\\nClassification Report:')\n",
        "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "    \n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'inference_time_ms': avg_inference_time,\n",
        "        'y_true': y_true,\n",
        "        'y_pred': y_pred\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 11: Load Data and Create Data Loaders (SAME AS YOUR NOTEBOOK)\n",
        "# =============================================\n",
        "print('Loading datasets...')\n",
        "\n",
        "# Load data with raw audio for Wav2Vec2/HuBERT\n",
        "features, raw_audios, labels, le = load_data(extract_raw=True)\n",
        "print(f'Datasets loaded: {features.shape[0]} samples')\n",
        "\n",
        "# Split data - SAME as your notebook (80/10/10)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# Split raw audio with same indices\n",
        "raw_train, raw_temp, _, _ = train_test_split(\n",
        "    raw_audios, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "raw_val, raw_test, _, _ = train_test_split(\n",
        "    raw_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f'Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}')\n",
        "\n",
        "# Create DataLoaders for MFCC models\n",
        "train_dataset = EmotionDataset(X_train, y_train)\n",
        "val_dataset = EmotionDataset(X_val, y_val)\n",
        "test_dataset = EmotionDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Create DataLoaders for raw audio models\n",
        "raw_train_dataset = RawAudioDataset(raw_train, y_train)\n",
        "raw_val_dataset = RawAudioDataset(raw_val, y_val)\n",
        "raw_test_dataset = RawAudioDataset(raw_test, y_test)\n",
        "\n",
        "raw_train_loader = DataLoader(raw_train_dataset, batch_size=8, shuffle=True)  # Smaller batch for large models\n",
        "raw_val_loader = DataLoader(raw_val_dataset, batch_size=8, shuffle=False)\n",
        "raw_test_loader = DataLoader(raw_test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "num_classes = len(le.classes_)\n",
        "input_size = N_MFCC\n",
        "hidden_size = 128\n",
        "num_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 12: Train and Evaluate All Models\n",
        "# =============================================\n",
        "results = []\n",
        "\n",
        "# Model 1: Your Original CNN-BiLSTM\n",
        "print('\\n' + '='*60)\n",
        "print('Training Model 1: CNN-BiLSTM (Your Original)')\n",
        "print('='*60)\n",
        "model1 = EmotionModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model1.parameters(), lr=LEARNING_RATE)\n",
        "train_model(model1, train_loader, val_loader, criterion, optimizer, EPOCHS, 'CNN_BiLSTM')\n",
        "model1.load_state_dict(torch.load('CNN_BiLSTM_best.pth'))\n",
        "results.append(evaluate_model(model1, test_loader, le, 'CNN-BiLSTM'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Model 2: CNN-Transformer Hybrid\n",
        "print('\\n' + '='*60)\n",
        "print('Training Model 2: CNN-Transformer Hybrid')\n",
        "print('='*60)\n",
        "model2 = CNNTransformerHybrid(num_classes).to(device)\n",
        "optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
        "train_model(model2, train_loader, val_loader, criterion, optimizer, EPOCHS, 'CNN_Transformer')\n",
        "model2.load_state_dict(torch.load('CNN_Transformer_best.pth'))\n",
        "results.append(evaluate_model(model2, test_loader, le, 'CNN-Transformer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Model 3: ECAPA-TDNN\n",
        "print('\\n' + '='*60)\n",
        "print('Training Model 3: ECAPA-TDNN')\n",
        "print('='*60)\n",
        "model3 = ECAPA_TDNN(input_size=N_MFCC, num_classes=num_classes).to(device)\n",
        "optimizer = optim.Adam(model3.parameters(), lr=LEARNING_RATE)\n",
        "train_model(model3, train_loader, val_loader, criterion, optimizer, EPOCHS, 'ECAPA_TDNN')\n",
        "model3.load_state_dict(torch.load('ECAPA_TDNN_best.pth'))\n",
        "results.append(evaluate_model(model3, test_loader, le, 'ECAPA-TDNN'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Model 4: Wav2Vec2 (if transformers available)\n",
        "if TRANSFORMERS_AVAILABLE:\n",
        "    print('\\n' + '='*60)\n",
        "    print('Training Model 4: Wav2Vec2')\n",
        "    print('='*60)\n",
        "    model4 = Wav2Vec2ForSER(num_classes=num_classes).to(device)\n",
        "    optimizer = optim.Adam(model4.parameters(), lr=1e-4)  # Lower LR for pretrained\n",
        "    train_model(model4, raw_train_loader, raw_val_loader, criterion, optimizer, 20, 'Wav2Vec2')  # Fewer epochs\n",
        "    model4.load_state_dict(torch.load('Wav2Vec2_best.pth'))\n",
        "    results.append(evaluate_model(model4, raw_test_loader, le, 'Wav2Vec2'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Model 5: HuBERT (if transformers available)\n",
        "if TRANSFORMERS_AVAILABLE:\n",
        "    print('\\n' + '='*60)\n",
        "    print('Training Model 5: HuBERT')\n",
        "    print('='*60)\n",
        "    model5 = HuBERTForSER(num_classes=num_classes).to(device)\n",
        "    optimizer = optim.Adam(model5.parameters(), lr=1e-4)\n",
        "    train_model(model5, raw_train_loader, raw_val_loader, criterion, optimizer, 20, 'HuBERT')\n",
        "    model5.load_state_dict(torch.load('HuBERT_best.pth'))\n",
        "    results.append(evaluate_model(model5, raw_test_loader, le, 'HuBERT'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 13: Results Comparison Table\n",
        "# =============================================\n",
        "import pandas as pd\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame([{\n",
        "    'Model': r['model'],\n",
        "    'Accuracy (%)': f\"{r['accuracy']:.2f}\",\n",
        "    'F1-Score (%)': f\"{r['f1_score']:.2f}\",\n",
        "    'Precision (%)': f\"{r['precision']:.2f}\",\n",
        "    'Recall (%)': f\"{r['recall']:.2f}\",\n",
        "    'Inference (ms)': f\"{r['inference_time_ms']:.2f}\"\n",
        "} for r in results])\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('BENCHMARK RESULTS SUMMARY')\n",
        "print('='*80)\n",
        "print(results_df.to_string(index=False))\n",
        "results_df.to_csv('benchmark_results.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 14: Visualization - Performance Comparison\n",
        "# =============================================\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "models = [r['model'] for r in results]\n",
        "colors = plt.cm.Set2(np.linspace(0, 1, len(models)))\n",
        "\n",
        "# Accuracy comparison\n",
        "ax1 = axes[0, 0]\n",
        "accuracies = [r['accuracy'] for r in results]\n",
        "bars1 = ax1.bar(models, accuracies, color=colors)\n",
        "ax1.set_ylabel('Accuracy (%)')\n",
        "ax1.set_title('Model Accuracy Comparison')\n",
        "ax1.set_ylim([0, 100])\n",
        "for bar, acc in zip(bars1, accuracies):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{acc:.1f}%', \n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# F1-Score comparison\n",
        "ax2 = axes[0, 1]\n",
        "f1_scores = [r['f1_score'] for r in results]\n",
        "bars2 = ax2.bar(models, f1_scores, color=colors)\n",
        "ax2.set_ylabel('F1-Score (%)')\n",
        "ax2.set_title('Model F1-Score Comparison')\n",
        "ax2.set_ylim([0, 100])\n",
        "for bar, f1 in zip(bars2, f1_scores):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{f1:.1f}%', \n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Inference time comparison\n",
        "ax3 = axes[1, 0]\n",
        "inf_times = [r['inference_time_ms'] for r in results]\n",
        "bars3 = ax3.bar(models, inf_times, color=colors)\n",
        "ax3.set_ylabel('Inference Time (ms/sample)')\n",
        "ax3.set_title('Model Inference Speed Comparison')\n",
        "for bar, t in zip(bars3, inf_times):\n",
        "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, f'{t:.2f}', \n",
        "             ha='center', va='bottom', fontsize=9)\n",
        "ax3.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Radar chart for overall comparison\n",
        "ax4 = axes[1, 1]\n",
        "ax4.remove()\n",
        "ax4 = fig.add_subplot(2, 2, 4, projection='polar')\n",
        "\n",
        "categories = ['Accuracy', 'F1-Score', 'Precision', 'Recall']\n",
        "N = len(categories)\n",
        "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "for i, r in enumerate(results):\n",
        "    values = [r['accuracy'], r['f1_score'], r['precision'], r['recall']]\n",
        "    values += values[:1]\n",
        "    ax4.plot(angles, values, 'o-', linewidth=2, label=r['model'], color=colors[i])\n",
        "    ax4.fill(angles, values, alpha=0.1, color=colors[i])\n",
        "\n",
        "ax4.set_xticks(angles[:-1])\n",
        "ax4.set_xticklabels(categories)\n",
        "ax4.set_ylim([0, 100])\n",
        "ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "ax4.set_title('Overall Performance Radar')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('benchmark_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 15: Confusion Matrices for All Models\n",
        "# =============================================\n",
        "n_models = len(results)\n",
        "fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n",
        "\n",
        "if n_models == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for ax, r in zip(axes, results):\n",
        "    cm = confusion_matrix(r['y_true'], r['y_pred'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=le.classes_, yticklabels=le.classes_, ax=ax)\n",
        "    ax.set_title(f\"{r['model']}\\nAcc: {r['accuracy']:.1f}%\")\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('True')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrices.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 16: Statistical Significance Test (McNemar's Test)\n",
        "# =============================================\n",
        "from scipy.stats import chi2\n",
        "\n",
        "def mcnemar_test(y_true, y_pred1, y_pred2):\n",
        "    \"\"\"Perform McNemar's test between two models\"\"\"\n",
        "    correct1 = np.array(y_pred1) == np.array(y_true)\n",
        "    correct2 = np.array(y_pred2) == np.array(y_true)\n",
        "    \n",
        "    # b: model1 correct, model2 wrong\n",
        "    b = np.sum(correct1 & ~correct2)\n",
        "    # c: model1 wrong, model2 correct\n",
        "    c = np.sum(~correct1 & correct2)\n",
        "    \n",
        "    if b + c == 0:\n",
        "        return 1.0  # No difference\n",
        "    \n",
        "    # McNemar's test statistic\n",
        "    statistic = (abs(b - c) - 1) ** 2 / (b + c)\n",
        "    p_value = 1 - chi2.cdf(statistic, df=1)\n",
        "    return p_value\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('Statistical Significance Tests (McNemar)')\n",
        "print('='*60)\n",
        "\n",
        "if len(results) > 1:\n",
        "    y_true = results[0]['y_true']\n",
        "    for i in range(len(results)):\n",
        "        for j in range(i+1, len(results)):\n",
        "            p_val = mcnemar_test(y_true, results[i]['y_pred'], results[j]['y_pred'])\n",
        "            sig = '***' if p_val < 0.001 else '**' if p_val < 0.01 else '*' if p_val < 0.05 else ''\n",
        "            print(f\"{results[i]['model']} vs {results[j]['model']}: p={p_val:.4f} {sig}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================\n",
        "# CELL 17: Final Summary\n",
        "# =============================================\n",
        "print('\\n' + '='*80)\n",
        "print('FINAL BENCHMARK SUMMARY')\n",
        "print('='*80)\n",
        "\n",
        "# Find best model\n",
        "best_acc_idx = np.argmax([r['accuracy'] for r in results])\n",
        "best_f1_idx = np.argmax([r['f1_score'] for r in results])\n",
        "fastest_idx = np.argmin([r['inference_time_ms'] for r in results])\n",
        "\n",
        "print(f\"\\nBest Accuracy: {results[best_acc_idx]['model']} ({results[best_acc_idx]['accuracy']:.2f}%)\")\n",
        "print(f\"Best F1-Score: {results[best_f1_idx]['model']} ({results[best_f1_idx]['f1_score']:.2f}%)\")\n",
        "print(f\"Fastest Model: {results[fastest_idx]['model']} ({results[fastest_idx]['inference_time_ms']:.2f} ms/sample)\")\n",
        "\n",
        "print('\\n' + '='*80)\n",
        "print('Results saved to: benchmark_results.csv')\n",
        "print('Plots saved to: benchmark_comparison.png, confusion_matrices.png')\n",
        "print('='*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}