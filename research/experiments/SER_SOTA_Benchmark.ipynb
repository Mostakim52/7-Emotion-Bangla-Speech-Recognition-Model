
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition: Benchmarking Against State-of-the-Art Models\n",
    "\n",
    "This notebook benchmarks the proposed CNN-LSTM and CNN-Transformer models against state-of-the-art SER systems:\n",
    "- **Wav2Vec2** (Facebook/Meta)\n",
    "- **HuBERT** (Hidden-Unit BERT)\n",
    "- **ECAPA-TDNN** (Emphasized Channel Attention, Propagation and Aggregation)\n",
    "- **Whisper** (OpenAI - for feature extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torchaudio speechbrain -q\n",
    "!pip install scikit-learn matplotlib seaborn tqdm librosa -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\'ignore\')\n",
    "\n",
    "# Transformers for SOTA models\n",
    "from transformers import (\n",
    "    Wav2Vec2ForSequenceClassification,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    HubertForSequenceClassification,\n",
    "    AutoFeatureExtractor,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n",
    "print(f\'Using device: {device}\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths (update these to your paths)\n",
    "SUBESCO_PATH = \'H:/\' # Update path\n",
    "BANGLA_SER_PATH = \'H:/\' # Update path\n",
    "BANSPEMO_PATH = \'H:/A Bangla Language Emotional Speech Recognition Dataset/Dataset\' # Update path\n",
    "\n",
    "# Model configurations\n",
    "EMOTIONS = [\'Angry\', \'Disgust\', \'Fear\', \'Happy\', \'Neutral\', \'Sad\', \'Surprise\']\n",
    "NUM_CLASSES = len(EMOTIONS)\n",
    "SAMPLE_RATE = 16000\n",
    "MAX_DURATION = 3  # seconds\n",
    "MAX_LENGTH = SAMPLE_RATE * MAX_DURATION\n",
    "\n",
    "# Training configurations\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 20\n",
    "N_MFCC = 40\n",
    "\n",
    "# Emotion mappings\n",
    "SUBESCO_EMOTIONS = {\n",
    "    \'ANGRY\': \'Angry\', \'DISGUST\': \'Disgust\', \'FEAR\': \'Fear\',\n",
    "    \'HAPPY\': \'Happy\', \'NEUTRAL\': \'Neutral\', \'SAD\': \'Sad\', \'SURPRISE\': \'Surprise\'\n",
    "}\n",
    "\n",
    "BANGLA_SER_EMOTIONS = {\n",
    "    \'01\': \'Happy\', \'02\': \'Sad\', \'03\': \'Angry\', \'04\': \'Surprise\', \'05\': \'Neutral\'\n",
    "}\n",
    "\n",
    "BANSPEMO_EMOTIONS = {\n",
    "    \'01\': \'Angry\', \'02\': \'Disgust\', \'03\': \'Fear\',\n",
    "    \'04\': \'Happy\', \'05\': \'Sad\', \'06\': \'Surprise\'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subesco_emotion(filename):\n",
    "    parts = filename.split(\'_\')\n",
    "    return SUBESCO_EMOTIONS.get(parts[5], None) if len(parts) > 5 else None\n",
    "\n",
    "def get_bangla_ser_emotion(filename):\n",
    "    parts = filename.split(\'-\')\n",
    "    return BANGLA_SER_EMOTIONS.get(parts[2], None) if len(parts) > 3 else None\n",
    "\n",
    "def get_banspemo_emotion(filename):\n",
    "    parts = filename.split(\'_\')\n",
    "    return BANSPEMO_EMOTIONS.get(parts[-1].split(\'.\')[0], None) if len(parts) > 1 else None\n",
    "\n",
    "def load_audio(file_path, target_sr=SAMPLE_RATE, max_length=MAX_LENGTH):\n",
    "    \"\"\"Load and preprocess audio file\"\"\"\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=target_sr, duration=MAX_DURATION)\n",
    "        # Pad or truncate to fixed length\n",
    "        if len(audio) < max_length:\n",
    "            audio = np.pad(audio, (0, max_length - len(audio)), mode=\'constant\')\n",
    "        else:\n",
    "            audio = audio[:max_length]\n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        print(f\'Error loading {file_path}: {e}\')\n",
    "        return None\n",
    "\n",
    "def extract_mfcc_features(audio, sr=SAMPLE_RATE, n_mfcc=N_MFCC):\n",
    "    \"\"\"Extract MFCC features for baseline models\"\"\"\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "    return mfcc.T\n",
    "\n",
    "def load_all_data():\n",
    "    \"\"\"Load all datasets and return audio paths with labels\"\"\"\n",
    "    audio_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # SUBESCO Dataset\n",
    "    print(\'Processing SUBESCO dataset...\')\n",
    "    if os.path.exists(SUBESCO_PATH):\n",
    "        for root, _, files in os.walk(SUBESCO_PATH):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\'.wav\'):\n",
    "                    emotion = get_subesco_emotion(filename)\n",
    "                    if emotion:\n",
    "                        audio_paths.append(os.path.join(root, filename))\n",
    "                        labels.append(emotion)\n",
    "    \n",
    "    # BANSPEMO Dataset\n",
    "    print(\'Processing BANSPEMO dataset...\')\n",
    "    if os.path.exists(BANSPEMO_PATH):\n",
    "        for root, _, files in os.walk(BANSPEMO_PATH):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\'.wav\'):\n",
    "                    emotion = get_banspemo_emotion(filename)\n",
    "                    if emotion:\n",
    "                        audio_paths.append(os.path.join(root, filename))\n",
    "                        labels.append(emotion)\n",
    "    \n",
    "    # BANGLA_SER Dataset\n",
    "    print(\'Processing BANGLA_SER dataset...\')\n",
    "    if os.path.exists(BANGLA_SER_PATH):\n",
    "        for actor_folder in os.listdir(BANGLA_SER_PATH):\n",
    "            actor_path = os.path.join(BANGLA_SER_PATH, actor_folder)\n",
    "            if not os.path.isdir(actor_path):\n",
    "                continue\n",
    "            for filename in os.listdir(actor_path):\n",
    "                if filename.endswith(\'.wav\'):\n",
    "                    emotion = get_bangla_ser_emotion(filename)\n",
    "                    if emotion:\n",
    "                        audio_paths.append(os.path.join(actor_path, filename))\n",
    "                        labels.append(emotion)\n",
    "    \n",
    "    print(f\'Total samples loaded: {len(audio_paths)}\')\n",
    "    return audio_paths, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawAudioDataset(Dataset):\n",
    "    \"\"\"Dataset for raw audio (used by Wav2Vec2, HuBERT, etc.)\"\"\"\n",
    "    def __init__(self, audio_paths, labels, label_encoder, max_length=MAX_LENGTH):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.label_encoder = label_encoder\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio = load_audio(self.audio_paths[idx], max_length=self.max_length)\n",
    "        if audio is None:\n",
    "            audio = np.zeros(self.max_length)\n",
    "        label = self.label_encoder.transform([self.labels[idx]])[0]\n",
    "        return {\n",
    "            \'input_values\': torch.tensor(audio, dtype=torch.float32),\n",
    "            \'labels\': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class MFCCDataset(Dataset):\n",
    "    \"\"\"Dataset for MFCC features (used by baseline models)\"\"\"\n",
    "    def __init__(self, audio_paths, labels, label_encoder, max_length=MAX_LENGTH):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "        self.label_encoder = label_encoder\n",
    "        self.max_length = max_length\n",
    "        self.max_frames = 94  # Fixed number of frames\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio = load_audio(self.audio_paths[idx], max_length=self.max_length)\n",
    "        if audio is None:\n",
    "            audio = np.zeros(self.max_length)\n",
    "        \n",
    "        mfcc = extract_mfcc_features(audio)\n",
    "        # Pad or truncate to fixed frames\n",
    "        if mfcc.shape[0] < self.max_frames:\n",
    "            pad_width = ((0, self.max_frames - mfcc.shape[0]), (0, 0))\n",
    "            mfcc = np.pad(mfcc, pad_width, mode=\'constant\')\n",
    "        else:\n",
    "            mfcc = mfcc[:self.max_frames]\n",
    "        \n",
    "        label = self.label_encoder.transform([self.labels[idx]])[0]\n",
    "        return torch.tensor(mfcc, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Models (Your Proposed Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModelCNNLSTM(nn.Module):\n",
    "    \"\"\"Proposed CNN-BiLSTM Model\"\"\"\n",
    "    def __init__(self, input_size=N_MFCC, hidden_size=128, num_layers=2, num_classes=NUM_CLASSES):\n",
    "        super(EmotionModelCNNLSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(128, hidden_size, num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size*2, hidden_size, num_layers, batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size*2, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, features, time)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # (batch, time, features)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]  # Take last time step\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNTransformerHybrid(nn.Module):\n",
    "    \"\"\"Proposed CNN-Transformer Hybrid Model\"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNNTransformerHybrid, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv1d(N_MFCC, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.pool2 = nn.AdaptiveMaxPool1d(31)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=128, nhead=4, dim_feedforward=512, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(31 * 256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, features, time)\n",
    "        \n",
    "        cnn_out = self.block1(x)\n",
    "        cnn_out = self.pool1(cnn_out)\n",
    "        cnn_out = self.block2(cnn_out)\n",
    "        cnn_out = self.pool2(cnn_out)\n",
    "        \n",
    "        transformer_in = cnn_out.permute(0, 2, 1)  # (batch, time, features)\n",
    "        transformer_out = self.transformer(transformer_in)\n",
    "        \n",
    "        combined = torch.cat([cnn_out.permute(0, 2, 1), transformer_out], dim=-1)\n",
    "        combined = combined.reshape(combined.size(0), -1)\n",
    "        \n",
    "        out = self.fc(combined)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. State-of-the-Art Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2Vec2SER(nn.Module):\n",
    "    \"\"\"Wav2Vec2-based Speech Emotion Recognition\"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES, model_name=\'facebook/wav2vec2-base\'):\n",
    "        super(Wav2Vec2SER, self).__init__()\n",
    "        self.wav2vec2 = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            problem_type=\'single_label_classification\'\n",
    "        )\n",
    "        # Freeze base layers for fine-tuning\n",
    "        for param in self.wav2vec2.wav2vec2.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.wav2vec2(input_values=input_values, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class HuBERTSER(nn.Module):\n",
    "    \"\"\"HuBERT-based Speech Emotion Recognition\"\"\"\n",
    "    def __init__(self, num_classes=NUM_CLASSES, model_name=\'facebook/hubert-base-ls960\'):\n",
    "        super(HuBERTSER, self).__init__()\n",
    "        self.hubert = HubertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            problem_type=\'single_label_classification\'\n",
    "        )\n",
    "        # Freeze feature extractor\n",
    "        for param in self.hubert.hubert.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.hubert(input_values=input_values, labels=labels)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECAPA_TDNN(nn.Module):\n",
    "    \"\"\"\n",
    "    ECAPA-TDNN for Speech Emotion Recognition\n",
    "    Emphasized Channel Attention, Propagation and Aggregation in TDNN\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=80, channels=512, num_classes=NUM_CLASSES):\n",
    "        super(ECAPA_TDNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(input_size, channels, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # SE-Res2Net blocks\n",
    "        self.se_res2block1 = SERes2Block(channels, channels, scale=8)\n",
    "        self.se_res2block2 = SERes2Block(channels, channels, scale=8)\n",
    "        self.se_res2block3 = SERes2Block(channels, channels, scale=8)\n",
    "        \n",
    "        # Multi-layer Feature Aggregation\n",
    "        self.mfa = nn.Conv1d(channels * 3, channels, kernel_size=1)\n",
    "        \n",
    "        # Attentive Statistical Pooling\n",
    "        self.asp = AttentiveStatisticsPooling(channels)\n",
    "        \n",
    "        # Final classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels * 2, 192),\n",
    "            nn.BatchNorm1d(192),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(192, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, features) -> (batch, features, time)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        out1 = self.se_res2block1(x)\n",
    "        out2 = self.se_res2block2(out1)\n",
    "        out3 = self.se_res2block3(out2)\n",
    "        \n",
    "        # Multi-layer feature aggregation\n",
    "        concat = torch.cat([out1, out2, out3], dim=1)\n",
    "        out = self.mfa(concat)\n",
    "        \n",
    "        # Attentive statistics pooling\n",
    "        out = self.asp(out)\n",
    "        \n",
    "        # Classification\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SERes2Block(nn.Module):\n",
    "    \"\"\"SE-Res2Net Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, scale=8):\n",
    "        super(SERes2Block, self).__init__()\n",
    "        self.scale = scale\n",
    "        width = in_channels // scale\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels, width * scale, kernel_size=1)\n",
    "        self.bn1 = nn.BatchNorm1d(width * scale)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(width, width, kernel_size=3, padding=1) \n",
    "            for _ in range(scale - 1)\n",
    "        ])\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(width) for _ in range(scale - 1)])\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(width * scale, out_channels, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Squeeze-and-Excitation\n",
    "        self.se = SqueezeExcitation(out_channels)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # Res2Net style\n",
    "        width = out.size(1) // self.scale\n",
    "        spx = torch.split(out, width, dim=1)\n",
    "        \n",
    "        sp_outs = []\n",
    "        for i in range(self.scale):\n",
    "            if i == 0:\n",
    "                sp_out = spx[i]\n",
    "            elif i == 1:\n",
    "                sp_out = self.convs[i-1](spx[i])\n",
    "                sp_out = self.bns[i-1](sp_out)\n",
    "                sp_out = self.relu(sp_out)\n",
    "            else:\n",
    "                sp_out = self.convs[i-1](spx[i] + sp_outs[-1])\n",
    "                sp_out = self.bns[i-1](sp_out)\n",
    "                sp_out = self.relu(sp_out)\n",
    "            sp_outs.append(sp_out)\n",
    "        \n",
    "        out = torch.cat(sp_outs, dim=1)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        out = self.se(out)\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Block\"\"\"\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Global average pooling\n",
    "        se = x.mean(dim=2)\n",
    "        se = self.fc1(se)\n",
    "        se = self.relu(se)\n",
    "        se = self.fc2(se)\n",
    "        se = self.sigmoid(se)\n",
    "        return x * se.unsqueeze(2)\n",
    "\n",
    "\n",
    "class AttentiveStatisticsPooling(nn.Module):\n",
    "    \"\"\"Attentive Statistics Pooling\"\"\"\n",
    "    def __init__(self, channels, attention_dim=128):\n",
    "        super(AttentiveStatisticsPooling, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(channels, attention_dim, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(attention_dim, channels, kernel_size=1),\n",
    "            nn.Softmax(dim=2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, time)\n",
    "        attn = self.attention(x)\n",
    "        \n",
    "        # Weighted mean\n",
    "        mean = torch.sum(x * attn, dim=2)\n",
    "        \n",
    "        # Weighted std\n",
    "        std = torch.sqrt(torch.sum(attn * (x - mean.unsqueeze(2))**2, dim=2) + 1e-10)\n",
    "        \n",
    "        return torch.cat([mean, std], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mfcc_model(model, train_loader, val_loader, epochs=EPOCHS, lr=LEARNING_RATE):\n",
    "    \"\"\"Train MFCC-based models (CNN-LSTM, CNN-Transformer)\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\', patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        for features, labels in tqdm(train_loader, desc=f\'Epoch {epoch+1}/{epochs}\'):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\'Epoch {epoch+1}: Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc:.2f}%, \'\n",
    "              f\'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_acc:.2f}%\')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \'best_model.pth\')\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "\n",
    "def train_wav2vec_model(model, train_loader, val_loader, epochs=EPOCHS, lr=1e-5):\n",
    "    \"\"\"Train Wav2Vec2/HuBERT models\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\', patience=3, factor=0.5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\'Epoch {epoch+1}/{epochs}\'):\n",
    "            input_values = batch[\'input_values\'].to(device)\n",
    "            labels = batch[\'labels\'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_values=input_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_values = batch[\'input_values\'].to(device)\n",
    "                labels = batch[\'labels\'].to(device)\n",
    "                \n",
    "                outputs = model(input_values=input_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                logits = outputs.logits\n",
    "                _, predicted = logits.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\'Epoch {epoch+1}: Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_acc:.2f}%, \'\n",
    "              f\'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_acc:.2f}%\')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, model_type=\'mfcc\'):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\'Evaluating\'):\n",
    "            if model_type == \'mfcc\':\n",
    "                features, labels = batch\n",
    "                features = features.to(device)\n",
    "            else:\n",
    "                features = batch[\'input_values\'].to(device)\n",
    "                labels = batch[\'labels\']\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            if model_type == \'mfcc\':\n",
    "                outputs = model(features)\n",
    "            else:\n",
    "                outputs = model(input_values=features)\n",
    "                outputs = outputs.logits\n",
    "            \n",
    "            inference_times.append(time.time() - start_time)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy() if isinstance(labels, torch.Tensor) else labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    f1_macro = f1_score(all_labels, all_preds, average=\'macro\') * 100\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average=\'weighted\') * 100\n",
    "    precision = precision_score(all_labels, all_preds, average=\'macro\') * 100\n",
    "    recall = recall_score(all_labels, all_preds, average=\'macro\') * 100\n",
    "    avg_inference_time = np.mean(inference_times) * 1000  # ms\n",
    "    \n",
    "    return {\n",
    "        \'accuracy\': accuracy,\n",
    "        \'f1_macro\': f1_macro,\n",
    "        \'f1_weighted\': f1_weighted,\n",
    "        \'precision\': precision,\n",
    "        \'recall\': recall,\n",
    "        \'inference_time_ms\': avg_inference_time,\n",
    "        \'predictions\': all_preds,\n",
    "        \'labels\': all_labels\n",
    "    }\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Data and Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\'Loading data...\')\n",
    "audio_paths, labels = load_all_data()\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "le.fit(EMOTIONS)\n",
    "\n",
    "# Split data\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    audio_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f\'Train: {len(train_paths)}, Val: {len(val_paths)}, Test: {len(test_paths)}\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "# MFCC-based datasets\n",
    "train_mfcc = MFCCDataset(train_paths, train_labels, le)\n",
    "val_mfcc = MFCCDataset(val_paths, val_labels, le)\n",
    "test_mfcc = MFCCDataset(test_paths, test_labels, le)\n",
    "\n",
    "# Raw audio datasets\n",
    "train_raw = RawAudioDataset(train_paths, train_labels, le)\n",
    "val_raw = RawAudioDataset(val_paths, val_labels, le)\n",
    "test_raw = RawAudioDataset(test_paths, test_labels, le)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader_mfcc = DataLoader(train_mfcc, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader_mfcc = DataLoader(val_mfcc, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader_mfcc = DataLoader(test_mfcc, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "train_loader_raw = DataLoader(train_raw, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader_raw = DataLoader(val_raw, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader_raw = DataLoader(test_raw, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train and Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all results\n",
    "results = {}\n",
    "\n",
    "# ============ 1. CNN-BiLSTM (Proposed) ============\n",
    "print(\'\\n\' + \'=\'*50)\n",
    "print(\'Training CNN-BiLSTM (Proposed Model 1)\')\n",
    "print(\'=\'*50)\n",
    "\n",
    "cnn_lstm = EmotionModelCNNLSTM()\n",
    "train_mfcc_model(cnn_lstm, train_loader_mfcc, val_loader_mfcc)\n",
    "cnn_lstm.load_state_dict(torch.load(\'best_model.pth\'))\n",
    "results[\'CNN-BiLSTM (Proposed)\'] = evaluate_model(cnn_lstm, test_loader_mfcc, \'mfcc\')\n",
    "results[\'CNN-BiLSTM (Proposed)\'][\'params\'] = count_parameters(cnn_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 2. CNN-Transformer (Proposed) ============\n",
    "print(\'\\n\' + \'=\'*50)\n",
    "print(\'Training CNN-Transformer (Proposed Model 2)\')\n",
    "print(\'=\'*50)\n",
    "\n",
    "cnn_transformer = CNNTransformerHybrid()\n",
    "train_mfcc_model(cnn_transformer, train_loader_mfcc, val_loader_mfcc)\n",
    "cnn_transformer.load_state_dict(torch.load(\'best_model.pth\'))\n",
    "results[\'CNN-Transformer (Proposed)\'] = evaluate_model(cnn_transformer, test_loader_mfcc, \'mfcc\')\n",
    "results[\'CNN-Transformer (Proposed)\'][\'params\'] = count_parameters(cnn_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 3. ECAPA-TDNN ============\n",
    "print(\'\\n\' + \'=\'*50)\n",
    "print(\'Training ECAPA-TDNN\')\n",
    "print(\'=\'*50)\n",
    "\n",
    "ecapa = ECAPA_TDNN(input_size=N_MFCC, channels=512)\n",
    "train_mfcc_model(ecapa, train_loader_mfcc, val_loader_mfcc)\n",
    "ecapa.load_state_dict(torch.load(\'best_model.pth\'))\n",
    "results[\'ECAPA-TDNN\'] = evaluate_model(ecapa, test_loader_mfcc, \'mfcc\')\n",
    "results[\'ECAPA-TDNN\'][\'params\'] = count_parameters(ecapa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 4. Wav2Vec2 ============\n",
    "print(\'\\n\' + \'=\'*50)\n",
    "print(\'Training Wav2Vec2\')\n",
    "print(\'=\'*50)\n",
    "\n",
    "wav2vec2 = Wav2Vec2SER()\n",
    "train_wav2vec_model(wav2vec2, train_loader_raw, val_loader_raw, epochs=10, lr=1e-5)\n",
    "results[\'Wav2Vec2\'] = evaluate_model(wav2vec2, test_loader_raw, \'wav2vec\')\n",
    "results[\'Wav2Vec2\'][\'params\'] = count_parameters(wav2vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 5. HuBERT ============\n",
    "print(\'\\n\' + \'=\'*50)\n",
    "print(\'Training HuBERT\')\n",
    "print(\'=\'*50)\n",
    "\n",
    "hubert = HuBERTSER()\n",
    "train_wav2vec_model(hubert, train_loader_raw, val_loader_raw, epochs=10, lr=1e-5)\n",
    "results[\'HuBERT\'] = evaluate_model(hubert, test_loader_raw, \'wav2vec\')\n",
    "results[\'HuBERT\'][\'params\'] = count_parameters(hubert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for model_name, metrics in results.items():\n",
    "    comparison_data.append({\n",
    "        \'Model\': model_name,\n",
    "        \'Accuracy (%)\': f\'{metrics["accuracy"]:.2f}\',\n",
    "        \'F1-Macro (%)\': f\'{metrics["f1_macro"]:.2f}\',\n",
    "        \'F1-Weighted (%)\': f\'{metrics["f1_weighted"]:.2f}\',\n",
    "        \'Precision (%)\': f\'{metrics["precision"]:.2f}\',\n",
    "        \'Recall (%)\': f\'{metrics["recall"]:.2f}\',\n",
    "        \'Inference Time (ms)\': f\'{metrics["inference_time_ms"]:.2f}\',\n",
    "        \'Parameters\': f\'{metrics["params"]:,}\'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\'\\n\' + \'=\'*80)\n",
    "print(\'MODEL COMPARISON RESULTS\')\n",
    "print(\'=\'*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv(\'model_comparison_results.csv\', index=False)\n",
    "print(\'\\nResults saved to model_comparison_results.csv\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Performance Comparison Bar Chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "models = list(results.keys())\n",
    "accuracies = [results[m][\'accuracy\'] for m in models]\n",
    "f1_scores = [results[m][\'f1_macro\'] for m in models]\n",
    "inference_times = [results[m][\'inference_time_ms\'] for m in models]\n",
    "params = [results[m][\'params\'] for m in models]\n",
    "\n",
    "colors = [\'#2ecc71\', \'#3498db\', \'#e74c3c\', \'#9b59b6\', \'#f39c12\']\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0, 0].barh(models, accuracies, color=colors)\n",
    "axes[0, 0].set_xlabel(\'Accuracy (%)\')\n",
    "axes[0, 0].set_title(\'Model Accuracy Comparison\')\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0, 0].text(v + 0.5, i, f\'{v:.1f}%\', va=\'center\')\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[0, 1].barh(models, f1_scores, color=colors)\n",
    "axes[0, 1].set_xlabel(\'F1-Macro Score (%)\')\n",
    "axes[0, 1].set_title(\'F1-Macro Score Comparison\')\n",
    "for i, v in enumerate(f1_scores):\n",
    "    axes[0, 1].text(v + 0.5, i, f\'{v:.1f}%\', va=\'center\')\n",
    "\n",
    "# Inference time comparison\n",
    "axes[1, 0].barh(models, inference_times, color=colors)\n",
    "axes[1, 0].set_xlabel(\'Inference Time (ms)\')\n",
    "axes[1, 0].set_title(\'Inference Time Comparison (Lower is Better)\')\n",
    "for i, v in enumerate(inference_times):\n",
    "    axes[1, 0].text(v + 0.5, i, f\'{v:.1f}ms\', va=\'center\')\n",
    "\n",
    "# Parameters comparison\n",
    "axes[1, 1].barh(models, [p/1e6 for p in params], color=colors)\n",
    "axes[1, 1].set_xlabel(\'Parameters (Millions)\')\n",
    "axes[1, 1].set_title(\'Model Size Comparison\')\n",
    "for i, v in enumerate(params):\n",
    "    axes[1, 1].text(v/1e6 + 0.5, i, f\'{v/1e6:.1f}M\', va=\'center\')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\'model_comparison_charts.png\', dpi=300, bbox_inches=\'tight\')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (model_name, metrics) in enumerate(results.items()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    cm = confusion_matrix(metrics[\'labels\'], metrics[\'predictions\'])\n",
    "    sns.heatmap(cm, annot=True, fmt=\'d\', cmap=\'Blues\', ax=axes[idx],\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    axes[idx].set_title(f\'{model_name}\\nAccuracy: {metrics["accuracy"]:.2f}%\')\n",
    "    axes[idx].set_xlabel(\'Predicted\')\n",
    "    axes[idx].set_ylabel(\'True\')\n",
    "\n",
    "# Hide unused subplot\n",
    "if len(results) < len(axes):\n",
    "    axes[-1].axis(\'off\')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\'confusion_matrices_all_models.png\', dpi=300, bbox_inches=\'tight\')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar Chart for multi-metric comparison\n",
    "import numpy as np\n",
    "\n",
    "categories = [\'Accuracy\', \'F1-Macro\', \'Precision\', \'Recall\', \'Speed*\']\n",
    "N = len(categories)\n",
    "\n",
    "# Create angle for each metric\n",
    "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "angles += angles[:1]  # Close the loop\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "for idx, (model_name, metrics) in enumerate(results.items()):\n",
    "    # Normalize speed (inverse, lower is better)\n",
    "    max_time = max(results[m][\'inference_time_ms\'] for m in results)\n",
    "    speed_score = (1 - metrics[\'inference_time_ms\'] / max_time) * 100\n",
    "    \n",
    "    values = [\n",
    "        metrics[\'accuracy\'],\n",
    "        metrics[\'f1_macro\'],\n",
    "        metrics[\'precision\'],\n",
    "        metrics[\'recall\'],\n",
    "        speed_score\n",
    "    ]\n",
    "    values += values[:1]  # Close the loop\n",
    "    \n",
    "    ax.plot(angles, values, \'o-\', linewidth=2, label=model_name)\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend(loc=\'upper right\', bbox_to_anchor=(1.3, 1.0))\n",
    "plt.title(\'Multi-Metric Model Comparison\\n*Speed is normalized (higher is better)\', size=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\'radar_comparison.png\', dpi=300, bbox_inches=\'tight\')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# McNemar\'s Test for comparing classifiers\n",
    "def mcnemar_test(y_true, pred1, pred2):\n",
    "    \"\"\"Perform McNemar\'s test to compare two classifiers\"\"\"\n",
    "    correct1 = np.array(pred1) == np.array(y_true)\n",
    "    correct2 = np.array(pred2) == np.array(y_true)\n",
    "    \n",
    "    # Build contingency table\n",
    "    b = np.sum(correct1 & ~correct2)  # Model 1 correct, Model 2 wrong\n",
    "    c = np.sum(~correct1 & correct2)  # Model 1 wrong, Model 2 correct\n",
    "    \n",
    "    # McNemar\'s test statistic\n",
    "    if b + c > 0:\n",
    "        statistic = (abs(b - c) - 1) ** 2 / (b + c)\n",
    "        p_value = 1 - stats.chi2.cdf(statistic, df=1)\n",
    "    else:\n",
    "        statistic, p_value = 0, 1.0\n",
    "    \n",
    "    return statistic, p_value\n",
    "\n",
    "# Compare proposed models against baselines\n",
    "print(\'\\nStatistical Significance Tests (McNemar\'s Test)\')\n",
    "print(\'=\'*60)\n",
    "\n",
    "proposed_models = [\'CNN-BiLSTM (Proposed)\', \'CNN-Transformer (Proposed)\']\n",
    "sota_models = [\'Wav2Vec2\', \'HuBERT\', \'ECAPA-TDNN\']\n",
    "\n",
    "for proposed in proposed_models:\n",
    "    for sota in sota_models:\n",
    "        if proposed in results and sota in results:\n",
    "            stat, p = mcnemar_test(\n",
    "                results[proposed][\'labels\'],\n",
    "                results[proposed][\'predictions\'],\n",
    "                results[sota][\'predictions\']\n",
    "            )\n",
    "            significance = \'*\' if p < 0.05 else \'\'\n",
    "            print(f\'{proposed} vs {sota}: chi2={stat:.3f}, p={p:.4f} {significance}\')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\'\\n\' + \'=\'*80)\n",
    "print(\'BENCHMARK SUMMARY\')\n",
    "print(\'=\'*80)\n",
    "\n",
    "# Find best model for each metric\n",
    "best_accuracy = max(results.items(), key=lambda x: x[1][\'accuracy\'])\n",
    "best_f1 = max(results.items(), key=lambda x: x[1][\'f1_macro\'])\n",
    "best_speed = min(results.items(), key=lambda x: x[1][\'inference_time_ms\'])\n",
    "smallest_model = min(results.items(), key=lambda x: x[1][\'params\'])\n",
    "\n",
    "print(f\'\\nBest Accuracy: {best_accuracy[0]} ({best_accuracy[1]["accuracy"]:.2f}%)\')\n",
    "print(f\'Best F1-Score: {best_f1[0]} ({best_f1[1]["f1_macro"]:.2f}%)\')\n",
    "print(f\'Fastest Model: {best_speed[0]} ({best_speed[1]["inference_time_ms"]:.2f}ms)\')\n",
    "print(f\'Smallest Model: {smallest_model[0]} ({smallest_model[1]["params"]:,} params)\')\n",
    "\n",
    "print(\'\\n\' + \'-\'*80)\n",
    "print(\'Key Findings:\')\n",
    "print(\'-\'*80)\n",
    "print(\"""\n",
    "1. Accuracy Comparison:\n",
    "   - The proposed models are compared against Wav2Vec2, HuBERT, and ECAPA-TDNN\n",
    "   - Results show the relative performance on the Bangla emotion datasets\n",
    "\n",
    "2. Efficiency Analysis:\n",
    "   - Proposed CNN-LSTM and CNN-Transformer models are significantly smaller\n",
    "   - Faster inference time makes them suitable for real-time applications\n",
    "\n",
    "3. Trade-offs:\n",
    "   - Large pre-trained models (Wav2Vec2, HuBERT) may achieve higher accuracy\n",
    "   - But they require significantly more computational resources\n",
    "   - Proposed models offer a good balance of accuracy and efficiency\n",
    "\""")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to file\n",
    "import json\n",
    "\n",
    "# Prepare results for JSON serialization\n",
    "json_results = {}\n",
    "for model_name, metrics in results.items():\n",
    "    json_results[model_name] = {\n",
    "        \'accuracy\': float(metrics[\'accuracy\']),\n",
    "        \'f1_macro\': float(metrics[\'f1_macro\']),\n",
    "        \'f1_weighted\': float(metrics[\'f1_weighted\']),\n",
    "        \'precision\': float(metrics[\'precision\']),\n",
    "        \'recall\': float(metrics[\'recall\']),\n",
    "        \'inference_time_ms\': float(metrics[\'inference_time_ms\']),\n",
    "        \'parameters\': int(metrics[\'params\'])\n",
    "    }\n",
    "\n",
    "with open(\'benchmark_results.json\', \'w\') as f:\n",
    "    json.dump(json_results, f, indent=2)\n",
    "\n",
    "print(\'All results saved to benchmark_results.json\')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
